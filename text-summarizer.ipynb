{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:09:21.972414Z","iopub.status.busy":"2024-08-26T15:09:21.971670Z","iopub.status.idle":"2024-08-26T15:12:28.146069Z","shell.execute_reply":"2024-08-26T15:12:28.144948Z","shell.execute_reply.started":"2024-08-26T15:09:21.972380Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","kaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.33.0 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n","ydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.18 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n","bitsandbytes==0.41.1 chromadb==0.4.12 -q"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:12:28.148709Z","iopub.status.busy":"2024-08-26T15:12:28.148388Z","iopub.status.idle":"2024-08-26T15:12:36.800293Z","shell.execute_reply":"2024-08-26T15:12:36.799221Z","shell.execute_reply.started":"2024-08-26T15:12:28.148679Z"},"trusted":true},"outputs":[],"source":["from torch import cuda, bfloat16\n","import torch\n","import transformers\n","from transformers import AutoTokenizer\n","from time import time\n","#import chromadb\n","#from chromadb.config import Settings\n","from langchain.llms import HuggingFacePipeline\n","from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain.vectorstores import Chroma\n","from langchain import PromptTemplate, LLMChain"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:12:36.802091Z","iopub.status.busy":"2024-08-26T15:12:36.801627Z","iopub.status.idle":"2024-08-26T15:12:52.050903Z","shell.execute_reply":"2024-08-26T15:12:52.049563Z","shell.execute_reply.started":"2024-08-26T15:12:36.802064Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -q accelerate bitsandbytes\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:12:52.055093Z","iopub.status.busy":"2024-08-26T15:12:52.054626Z","iopub.status.idle":"2024-08-26T15:12:52.111977Z","shell.execute_reply":"2024-08-26T15:12:52.110823Z","shell.execute_reply.started":"2024-08-26T15:12:52.055057Z"},"trusted":true},"outputs":[],"source":["model_id = 'Manu1507/Llama-2-7b-finetune-summarizer'\n","\n","device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n","\n","# set quantization configuration to load large model with less GPU memory\n","# this requires the `bitsandbytes` library\n","bnb_config = transformers.BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=bfloat16\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:12:52.114496Z","iopub.status.busy":"2024-08-26T15:12:52.113724Z","iopub.status.idle":"2024-08-26T15:19:55.558513Z","shell.execute_reply":"2024-08-26T15:19:55.557421Z","shell.execute_reply.started":"2024-08-26T15:12:52.114452Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd23ee1c84d34010ba6006a107d301d5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-08-26 15:12:56.812137: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-26 15:12:56.812289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-26 15:12:56.958822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d284ee12473d43799f420754af7ffe56","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94818f4d9eb4a7aa76e0e7abf111402","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"660a64be0a674b16ae8363e6316f2b6f","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1056b837f66f4901805c6a890678ad77","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f6198486a644c42bded855c6ea4028b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a74a08ba76524857a5053c4390bbb44c","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3d2040c5fce4d28aa86ec2dcd9b92a6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05be096ad14342d3b42e2ee41efff072","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b41b62446b05450eb2e36275142433a1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e9f50bc7fca4a6b8b23c661d10ee05e","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9904588245ea40808a847b55db109ad9","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Prepare model, tokenizer: 423.413 sec.\n"]}],"source":["time_1 = time()\n","model_config = transformers.AutoConfig.from_pretrained(\n","    model_id,\n",")\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    trust_remote_code=True,\n","    config=model_config,\n","    quantization_config=bnb_config,\n","    device_map='auto',\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","time_2 = time()\n","print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:19:55.560496Z","iopub.status.busy":"2024-08-26T15:19:55.560095Z","iopub.status.idle":"2024-08-26T15:19:57.302269Z","shell.execute_reply":"2024-08-26T15:19:57.301203Z","shell.execute_reply.started":"2024-08-26T15:19:55.560468Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prepare pipeline: 1.736 sec.\n"]}],"source":["time_1 = time()\n","summarization_pipeline = transformers.pipeline(\n","    \"text-generation\", #task\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    max_length=1000,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id\n",")\n","time_2 = time()\n","print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:19:57.304279Z","iopub.status.busy":"2024-08-26T15:19:57.303648Z","iopub.status.idle":"2024-08-26T15:19:57.308905Z","shell.execute_reply":"2024-08-26T15:19:57.307911Z","shell.execute_reply.started":"2024-08-26T15:19:57.304252Z"},"trusted":true},"outputs":[],"source":["llm = HuggingFacePipeline(pipeline=summarization_pipeline, model_kwargs={'temperature': 0})"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:19:57.311025Z","iopub.status.busy":"2024-08-26T15:19:57.310350Z","iopub.status.idle":"2024-08-26T15:19:57.327414Z","shell.execute_reply":"2024-08-26T15:19:57.326369Z","shell.execute_reply.started":"2024-08-26T15:19:57.310997Z"},"trusted":true},"outputs":[],"source":["from langchain import PromptTemplate, LLMChain\n","\n","template = \"\"\"\n","Please summarize the following text in complete bullet points. Ensure that the summary covers all key points and is fully articulated:\n","{text}\n","BULLET POINT SUMMARY:\n","\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:19:57.329445Z","iopub.status.busy":"2024-08-26T15:19:57.328680Z","iopub.status.idle":"2024-08-26T15:19:57.341005Z","shell.execute_reply":"2024-08-26T15:19:57.340084Z","shell.execute_reply.started":"2024-08-26T15:19:57.329412Z"},"trusted":true},"outputs":[],"source":["text = \"\"\" As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI), a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models, further democratizing access in this important, fast-changing field.\n","\n","Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches, validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks. We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.\n","\n","Over the last year, large language models — natural language processing (NLP) systems with billions of parameters — have shown new capabilities to generate creative text, solve mathematical theorems, predict protein structures, answer reading comprehension questions, and more. They are one of the clearest cases of the substantial potential benefits AI can offer at scale to billions of people.\n","\n","Even with all the recent advancements in large language models, full research access to them remains limited because of the resources that are required to train and run such large models. This restricted access has limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues, such as bias, toxicity, and the potential for generating misinformation.\n","\n","Smaller models trained on more tokens — which are pieces of words — are easier to retrain and fine-tune for specific potential product use cases. We trained LLaMA 65B and LLaMA 33B on 1.4 trillion tokens. Our smallest model, LLaMA 7B, is trained on one trillion tokens.\n","\n","Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text. To train our model, we chose text from the 20 languages with the most speakers, focusing on those with Latin and Cyrillic alphabets.\n","\n","There is still more research that needs to be done to address the risks of bias, toxic comments, and hallucinations in large language models. Like other models, LLaMA shares these challenges. As a foundation model, LLaMA is designed to be versatile and can be applied to many different use cases, versus a fine-tuned model that is designed for a specific task. By sharing the code for LLaMA, other researchers can more easily test new approaches to limiting or eliminating these problems in large language models. We also provide in the paper a set of evaluations on benchmarks evaluating model biases and toxicity to show the model’s limitations and to support further research in this crucial area.\n","\n","To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license focused on research use cases. Access to the model will be granted on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world. People interested in applying for access can find the link to the application in our research paper.\n","\n","We believe that the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular. We look forward to seeing what the community can learn — and eventually build — using LLaMA.\n","\"\"\""]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:19:57.342548Z","iopub.status.busy":"2024-08-26T15:19:57.342244Z","iopub.status.idle":"2024-08-26T15:20:10.823619Z","shell.execute_reply":"2024-08-26T15:20:10.822685Z","shell.execute_reply.started":"2024-08-26T15:19:57.342522Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","• Large language models are a fast-changing field that can offer significant benefits at scale to billions of people. \n","\n","• However, training these models requires significant computing power and resources, which limits access.\n","\n","• Smaller foundation models, such as LLaMA, are easier to retrain and fine-tune for specific\n"]}],"source":["print(llm_chain.run(text))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:20:10.827418Z","iopub.status.busy":"2024-08-26T15:20:10.827104Z","iopub.status.idle":"2024-08-26T15:20:10.832687Z","shell.execute_reply":"2024-08-26T15:20:10.831712Z","shell.execute_reply.started":"2024-08-26T15:20:10.827391Z"},"trusted":true},"outputs":[],"source":["text = \"\"\"Artificial intelligence (AI) is a branch of computer science focused on developing intelligent machines that can mimic human cognitive \n","           functions like learning, problem-solving, anddecision-making. AI research has made significant progress in recent years, with \n","           advancementsin machine learning algorithms and the increasing availability of data. AI applications are nowwidespread, impacting various \n","           sectors like healthcare, finance, transportation, and manufacturing.From medical diagnosis tools to self-driving cars, AI is transforming \n","           industries and raisingethical considerations regarding automation, bias, and human control. As AI continues to evolve,it's crucial to \n","           develop responsible AI practices that ensure its benefits are harnessed for thebetterment of society.\"\"\""]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:20:10.834176Z","iopub.status.busy":"2024-08-26T15:20:10.833700Z","iopub.status.idle":"2024-08-26T15:20:27.104530Z","shell.execute_reply":"2024-08-26T15:20:27.103460Z","shell.execute_reply.started":"2024-08-26T15:20:10.834142Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[".\tIntroduction: AI has made significant progress in developing intelligent machines that can mimic human cognitive functions like learning and decision-making.\n",".\tAdvancement: AI research has made significant progress in recent years, with advancements in machine learning and the increasing availability of data.\n",".\tApplications: AI applications are now widespread, impacting various sectors like healthcare, finance, transportation, and manufacturing.\n",".\tTransforming industries: AI is transforming industries, raisingsome ethical considerations regarding automation, bias, and human control.\n",".\tHarnessing: As AI continues to evolve, its crucial to develop responsible AI practices that ensure its benefits are for the betterment of society.\n"]}],"source":["print(llm_chain.run(text))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:20:27.106164Z","iopub.status.busy":"2024-08-26T15:20:27.105853Z","iopub.status.idle":"2024-08-26T15:20:42.145082Z","shell.execute_reply":"2024-08-26T15:20:42.143820Z","shell.execute_reply.started":"2024-08-26T15:20:27.106132Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Collecting pyngrok\n","  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting flask_ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\n","Requirement already satisfied: Flask>=0.8 in /opt/conda/lib/python3.10/site-packages (from flask_ngrok) (3.0.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from flask_ngrok) (2.31.0)\n","Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.8->flask_ngrok) (3.0.2)\n","Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.8->flask_ngrok) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.8->flask_ngrok) (8.1.7)\n","Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.8->flask_ngrok) (1.7.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->flask_ngrok) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->flask_ngrok) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->flask_ngrok) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->flask_ngrok) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask>=0.8->flask_ngrok) (2.1.3)\n","Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n","Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Installing collected packages: pyngrok, flask_ngrok\n","Successfully installed flask_ngrok-0.0.25 pyngrok-7.2.0\n"]}],"source":["!pip install pyngrok flask_ngrok"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:20:42.147307Z","iopub.status.busy":"2024-08-26T15:20:42.146957Z","iopub.status.idle":"2024-08-26T15:20:44.071897Z","shell.execute_reply":"2024-08-26T15:20:44.070725Z","shell.execute_reply.started":"2024-08-26T15:20:42.147277Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n"]}],"source":["!ngrok authtoken \"YOUR AUTHTOKEN KEY\""]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:20:44.073870Z","iopub.status.busy":"2024-08-26T15:20:44.073527Z","iopub.status.idle":"2024-08-26T15:20:44.485831Z","shell.execute_reply":"2024-08-26T15:20:44.484958Z","shell.execute_reply.started":"2024-08-26T15:20:44.073841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["* ngrok tunnel \"NgrokTunnel: \"https://37b2-104-196-176-56.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n"]}],"source":["from pyngrok import ngrok\n","\n","# Connect to the local Flask server running on port 5000\n","public_url = ngrok.connect(addr=\"5000\")\n","print('* ngrok tunnel \"{}\" -> \"http://127.0.0.1:5000\"'.format(public_url))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:22:13.082045Z","iopub.status.busy":"2024-08-26T15:22:13.080798Z","iopub.status.idle":"2024-08-26T15:22:14.301314Z","shell.execute_reply":"2024-08-26T15:22:14.299994Z","shell.execute_reply.started":"2024-08-26T15:22:13.082005Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["%mkdir templates -p"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:22:14.303504Z","iopub.status.busy":"2024-08-26T15:22:14.303054Z","iopub.status.idle":"2024-08-26T15:22:14.312410Z","shell.execute_reply":"2024-08-26T15:22:14.311221Z","shell.execute_reply.started":"2024-08-26T15:22:14.303466Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing templates/index.html\n"]}],"source":["%%writefile templates/index.html\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title>Text Summarization</title>\n","</head>\n","<body>\n","    <h1>Text Summarization</h1>\n","    <form action=\"/summarize\" method=\"POST\">\n","        <label for=\"input_text\">Input Text:</label><br>\n","        <textarea id=\"input_text\" name=\"input_text\" rows=\"10\" cols=\"80\">{{ input_text }}</textarea><br><br>\n","        <input type=\"submit\" value=\"Summarize\">\n","    </form>\n","    {% if summary_text %}\n","    <h2>Summary:</h2>\n","    <textarea id=\"summary_text\" rows=\"10\" cols=\"80\">{{ summary_text }}</textarea><br><br>\n","    <p>Generation Time: {{ generation_time }} seconds</p>\n","    {% endif %}\n","</body>\n","</html>"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:22:14.314327Z","iopub.status.busy":"2024-08-26T15:22:14.313994Z","iopub.status.idle":"2024-08-26T15:28:24.575473Z","shell.execute_reply":"2024-08-26T15:28:24.574079Z","shell.execute_reply.started":"2024-08-26T15:22:14.314300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" * Running on NgrokTunnel: \"https://fea9-104-196-176-56.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n","Result type: <class 'str'>\n","Result content: \n","* Artificial Intelligence (AI) focuses on creating machines capable of performing tasks that typically require human intelligence.\n","* Narrow AI simulates human cognition to perform a specific task while General AI would enable machines to perform any intellectual task.\n","* AI has significant implications for industries such as healthcare, finance, transportation, and entertainment.\n","* AI brings opportunities for innovation and improvement, however it also raises ethics and privacy concerns.\n","* The development of AI depends on how it is regulated and integrated into society.\n"]}],"source":["from flask import Flask, request, jsonify, render_template\n","from pyngrok import ngrok\n","import time\n","\n","app = Flask(__name__)\n","\n","def get_summary(text):\n","    try:\n","        start_time = time.time()\n","        result = llm_chain.run(text)\n","        print(f\"Result type: {type(result)}\")  # Print the type to verify\n","        print(f\"Result content: {result}\")  # Print the content to understand the output\n","        end_time = time.time()\n","        generation_time = round(end_time - start_time, 2)\n","        return result, generation_time\n","    except Exception as e:\n","        print(f\"Exception: {e}\")\n","        return f\"An error occurred: {str(e)}\", 0\n","\n","@app.route('/')\n","def home():\n","    return render_template('index.html')\n","\n","@app.route('/summarize', methods=['POST'])\n","def summarize():\n","    try:\n","        input_text = request.form['input_text']\n","        summary, generation_time = get_summary(input_text)\n","        return render_template('index.html', input_text=input_text, summary_text=summary, generation_time=generation_time)\n","    except Exception as e:\n","        print(f\"Error occurred: {e}\")\n","        return render_template('index.html', input_text=input_text, summary_text=\"An error occurred.\", generation_time=0)\n","\n","# Get a public URL\n","public_url = ngrok.connect(5000)\n","print(f\" * Running on {public_url}\")\n","\n","if __name__ == '__main__':\n","    app.run()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:42:09.836211Z","iopub.status.busy":"2024-08-26T15:42:09.835817Z","iopub.status.idle":"2024-08-26T15:42:24.628167Z","shell.execute_reply":"2024-08-26T15:42:24.626772Z","shell.execute_reply.started":"2024-08-26T15:42:09.836181Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Collecting flask_cors\n","  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: Flask>=0.9 in /opt/conda/lib/python3.10/site-packages (from flask_cors) (3.0.3)\n","Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (3.0.2)\n","Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (8.1.7)\n","Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (1.7.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask>=0.9->flask_cors) (2.1.3)\n","Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n","Installing collected packages: flask_cors\n","Successfully installed flask_cors-4.0.1\n"]}],"source":["!pip install flask_cors"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T15:42:29.596957Z","iopub.status.busy":"2024-08-26T15:42:29.596007Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" * Running on NgrokTunnel: \"https://c117-104-196-176-56.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n","Result type: <class 'str'>\n","Result content: \n","* AI is a field of computer science focused on creating machines that can perform tasks that typically need human intelligence. \n","* AI is categorized into two types: Narrow AI and General AI. \n","* Narrow AI focuses on specific tasks and operates within a limited context. \n","* General AI would have the capacity to perform any intellectual task that a human can. \n","* AI has profound implications across industries, including health, finance, transportation, and entertainment. \n","* AI raises concerns about job displacement, privacy, and ethical implications. \n","* The future of AI will depend on how it is developed and integrated into society.\n","AI is being developed in a wide range of fields, including: \n","* Healthcare: AI is being used to diagnose diseases, develop personalized treatment plans, and predict patient outcomes. \n","* Finance: AI algorithms help with fraud detection, risk management, and automated trading. \n","* Transportation: AI is being used to create autonomous vehicles, optimize logistics, and improve transportation systems. \n","* Entertainment: AI is being used to create personalized content recommendations and even generate new forms of media. \n","The following are the top five most important AI trends to watch out for in the future: \n","* Adversarial training: This is a technique for training artificially intelligent (AI) systems to perform better on their tasks by pitting them against other AI systems, or adversarial systems. \n","* Explainable AI: Exp\n","Result type: <class 'str'>\n","Result content: \n","• AI simulates human cognitive functions in machines to perform tasks ranging from simple to complex.\n","• AI is categorized into Narrow AI (Weak AI) or General AI (Strong AI).\n","• Narrow AI operates within a limited context and cannot function beyond its programmed capabilities.\n","• General AI would possess the ability to perform any intellectual task a human can.\n","• AI can benefit industries such as healthcare, finance, transportation, and entertainment.\n","• AI raises concerns about job displacement, privacy, and ethics. \n","• AI will depend on how these technologies are developed and regulated.\n"]}],"source":["from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","from pyngrok import ngrok\n","import time\n","\n","app = Flask(__name__)\n","CORS(app)  # Enable CORS\n","\n","def get_summary(text):\n","    try:\n","        start_time = time.time()\n","        result = llm_chain.run(text)\n","        print(f\"Result type: {type(result)}\")\n","        print(f\"Result content: {result}\")\n","        end_time = time.time()\n","        generation_time = round(end_time - start_time, 2)\n","        return result, generation_time\n","    except Exception as e:\n","        print(f\"Exception: {e}\")\n","        return f\"An error occurred: {str(e)}\", 0\n","\n","@app.route('/summarize', methods=['POST'])\n","def summarize():\n","    try:\n","        data = request.get_json()\n","        input_text = data.get('input_text', '')\n","        summary, generation_time = get_summary(input_text)\n","        return jsonify({\n","            'input_text': input_text,\n","            'summary_text': summary,\n","            'generation_time': generation_time\n","        })\n","    except Exception as e:\n","        print(f\"Error occurred: {e}\")\n","        return jsonify({\n","            'input_text': '',\n","            'summary_text': 'An error occurred.',\n","            'generation_time': 0\n","        })\n","\n","# Get a public URL\n","public_url = ngrok.connect(5000)\n","print(f\" * Running on {public_url}\")\n","\n","if __name__ == '__main__':\n","    app.run()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1332934,"sourceId":2219619,"sourceType":"datasetVersion"},{"datasetId":4884056,"sourceId":8234785,"sourceType":"datasetVersion"},{"datasetId":4889139,"sourceId":8241861,"sourceType":"datasetVersion"},{"datasetId":4889150,"sourceId":8241874,"sourceType":"datasetVersion"},{"datasetId":4889169,"sourceId":8241895,"sourceType":"datasetVersion"},{"datasetId":4910415,"sourceId":8270647,"sourceType":"datasetVersion"},{"datasetId":5045202,"sourceId":8463148,"sourceType":"datasetVersion"},{"datasetId":5048074,"sourceId":8466953,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":235,"modelInstanceId":1462,"sourceId":1728,"sourceType":"modelInstanceVersion"},{"modelId":735,"modelInstanceId":3093,"sourceId":4298,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
